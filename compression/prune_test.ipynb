{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prune_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "hlmZ34WHn22k",
        "outputId": "48e664d0-33f4-43bf-f363-d6586c60d18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UL_kvSC0lTgm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vgg16 import vgg16, vgg16_bn, vgg_toy\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VJCGeo_qlTgu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TYPE = 'toy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "42tvwirapo8h",
        "outputId": "95bbf4b4-630e-42d2-f609-5668d3a38ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KMBwGHp_lTg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cfb7c3b-54ce-4e97-fc33-41c2853e8742"
      },
      "cell_type": "code",
      "source": [
        "class PruneConfig():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 1\n",
        "        self.lr = 0.01\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "        print(self.cuda)\n",
        "        self.seed = 42\n",
        "        self.log_rate = 10\n",
        "        self.log_file = \"log.txt\"\n",
        "        self.sensitivity = 2\n",
        "        self.debug = False\n",
        "cfg = PruneConfig()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I7UDpr-glTg6",
        "outputId": "9a81f140-254d-481d-a1ef-e5dbe3fd6c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(cfg.seed)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5c4480dfb0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3hPopuELlThC",
        "outputId": "7e35ef62-9f6f-412d-ca9e-b02036b50fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if cfg.cuda:\n",
        "    print(\"Using CUDA\")\n",
        "    torch.cuda.manual_seed(cfg.seed)\n",
        "else:\n",
        "    print(\"No CUDA\")\n",
        "kwargs = {'num_workers': 5, 'pin_memory': True} if cfg.cuda else {}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HKuWi06rlThH"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Toy MNIST Data\n",
        "**Pad to 224x244x1 since VGG16 originally takes in images of those size, so essentially this is just really really bad toy data**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "34G8Ojx0lThH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if TYPE == 'toy':\n",
        "    transform_list = [transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))]\n",
        "else:\n",
        "    transforms_list = [transforms.Pad(98), transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))]\n",
        "\n",
        "    \n",
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True,\n",
        "                                                         transform=transforms.Compose(transform_list)),\n",
        "                                          batch_size=cfg.batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, download=True,\n",
        "                                                         transform=transforms.Compose(transform_list)),\n",
        "                                          batch_size=cfg.batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RoCqANUMcrbv"
      },
      "cell_type": "markdown",
      "source": [
        "# TOY VGG"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-HthrHj0csq8",
        "outputId": "63bf89cd-807e-4ec0-ad49-adac00b5e9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dev  = torch.device(\"cuda\" if cfg.cuda else 'cpu')\n",
        "if TYPE == \"toy\":\n",
        "    model = vgg_toy(mask=True).to(dev)\n",
        "elif TYPE == \"bn\":\n",
        "    model = vgg16_bn(pretrained=True, mask=True, debug=cfg.debug, in_channels=3).to(dev)\n",
        "else:\n",
        "    model = vgg16(pretrained=True, mask=True, debug=cfg.debug, in_channels=3).to(dev)\n",
        "print(\"USING: \", TYPE)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USING:  toy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OxH4uvV3lThR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.lr, weight_decay=0.0001)\n",
        "optim_state_dict = optimizer.state_dict()\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W95uv9-OYx_z"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Hic9PHC6lThV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    tmp_loss = []\n",
        "    for epoch_i in range(epochs):\n",
        "        loss = 0\n",
        "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "        for batch_i, (x_in, y_in) in pbar:\n",
        "            if TYPE != 'toy':\n",
        "                x_in = torch.cat([x_in, x_in, x_in], dim=1)\n",
        "            x_in, y_in = x_in.to(dev), y_in.to(dev)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_in)\n",
        "            loss = criterion(output, y_in)\n",
        "            curr_loss = loss.item() / cfg.batch_size\n",
        "            loss.backward()\n",
        "            tmp_loss.append(curr_loss)\n",
        "            \n",
        "            # zero out pruned connections\n",
        "            for name, p in model.named_parameters():\n",
        "                if \"mask\" in name:\n",
        "                    continue\n",
        "                tensor = p.data.cpu().numpy()\n",
        "                grad_tensor = p.grad.data.cpu().numpy()\n",
        "                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
        "                p.grad.data = torch.from_numpy(grad_tensor).to(dev)\n",
        "            optimizer.step()\n",
        "            if batch_i % cfg.log_rate == 0:\n",
        "                done = batch_i * len(x_in)\n",
        "                percentage = 100. * batch_i / len(train_loader)\n",
        "                avg_loss = sum(tmp_loss)/len(tmp_loss)\n",
        "                tmp_loss = []\n",
        "                pbar.set_description(f\"Train Epoch: {epoch_i} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)] Loss: {avg_loss:.6f}\")\n",
        "                \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    curr_test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        tmp_loss = []\n",
        "        for data, target in test_loader:\n",
        "            if TYPE != 'toy':\n",
        "                data = torch.cat([data] * 3, dim=1)\n",
        "            data, target = data.to(dev), target.to(dev)\n",
        "            output = model(data)\n",
        "            test_loss = criterion(output, target)\n",
        "            curr_test_loss += test_loss.item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
        "        curr_test_loss /= len(test_loader.dataset)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "        print(f\"Test Set: Avg Loss: {curr_test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "E415UOxzlThY",
        "outputId": "8b78e97e-fac3-49d6-ac5e-ea4bd916064f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train(cfg.epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [15360/60000 ( 26%)] Loss: 0.053967:  27%|██▋       | 126/469 [01:46<04:47,  1.19it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_lCLWtMslThi",
        "outputId": "271373df-e64b-4d3b-9e87-004cc324ada4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set: Avg Loss: 0.4183, Accuracy: 2747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "E9XamnE6YP6y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model, f\"initial_model.ptmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sVST1eh-YvjN"
      },
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wSNjeL3DYt7i",
        "outputId": "0486075c-5e7b-4a30-f23e-f3bf905c3b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "cell_type": "code",
      "source": [
        "model.prune_by_std(cfg.sensitivity, debug=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c75df4a2098c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_by_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: prune_by_std() got an unexpected keyword argument 'debug'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J0sXWV99ZjTu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}